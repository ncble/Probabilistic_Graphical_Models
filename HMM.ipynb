{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Markov Model (HMM)\n",
    "\n",
    "Author: Lu Lin, 03/02/2019 This notebook has multiple objective:\n",
    "\n",
    "- Provide a highly parallelized, compact python implementation of HMM (e.g. \"No for-loop for non-sequential calculations\" by numpy tensor operations.)\n",
    "\n",
    "Denote \n",
    "- $S =$, $O$ the states and the observations spaces respectively; \n",
    "- $A$ the *transition probability* $a_{ij} = P(s_i \\rightarrow s_j)$ represents the probability of states transition; \n",
    "- $B$ the *emission probability* $b_i(o)$ the state $s_i$ emits observation $o$;\n",
    "- $\\pi$ initial state distribution\n",
    "- $\\lambda = (\\pi, A, B)$ the parameters of whole model\n",
    "\n",
    "In the following, I distinguish the discrete case and the continuous case of $S$ and $O$.\n",
    "\n",
    "### There are three basic problems of HMM:\n",
    "\n",
    "   1. **The Evaluation problem: (forward/backward messages)** Given $\\lambda = (\\pi, A, B)$ and a sequence of observation $O = (o_1, o_2, o_T)$, estimate $\\mathcal{P}(O|\\lambda)$\n",
    "      \n",
    "   2. **The Learning problem: (Baum-Welch algorithm, aka EM-algorithm)** Given $O = (o_1, o_2, o_T)$, estimate best fit parameters $\\lambda = (\\pi, A, B)$\n",
    "   3. **The Decoding problem: (Viterbi's algorithm)** Given $\\lambda = (\\pi, A, B)$ and a sequence of observation $O = (o_1, o_2, o_T)$, estimate hidden states.\n",
    "\n",
    "\n",
    "#### (1.) Foward/backward messages $\\alpha$, $\\beta$\n",
    "   Denote the forward/backward message as \n",
    "   - $\\alpha_t(i):= \\alpha_t(s_t=i) = P(o_1,...,o_t, s_t=i| \\lambda)$ \n",
    "   $$\\alpha_t(i) = b_i(o_{t+1}) \\cdot (\\sum_{j=1}^{s} \\alpha_t(j)\\cdot a_{ji}) $$\n",
    "   - $\\beta_t(i) := \\beta_t(s_t=i) =  P(o_t,...,o_T|s_t=i, \\lambda)$\n",
    "   $$\\beta_t(i) =  \\sum_{j=1}^{s} a_{ij}\\cdot b_j(o_{t+1})\\cdot \\beta_{t+1}(j) $$\n",
    "   \n",
    "   The probability $\\mathcal{P}(O|\\lambda)$ can be computed using forward or backward messages:\n",
    "   $$\\boxed{\\mathcal{P}(O|\\lambda) = \\sum_{i=1}^s \\alpha_T(i) = \\sum_{i=1}^s \\pi_i\\cdot b_i(o_1)\\cdot \\beta_1(i)}$$\n",
    "\n",
    "#### (2.) EM-algorithm\n",
    "   The goal is to use expectation-maximazation algorithm to find best fit parameters $\\lambda$. To do this, we introduce two parameters:\n",
    "   Denote \n",
    "   - $\\gamma_t(i) := P(s_t=i|O_{1:T}, \\lambda) = \\frac{P(s_t=i, O_{1:T}|\\lambda)}{P(O_{1:T}|\\lambda)}$\n",
    "   - $\\xi_t(i, j) := P(s_t=i, s_{t+1}=j|O_{1:T}, \\lambda)$\n",
    "   \n",
    "   Then, we have:\n",
    "   $$\\gamma_t(i) = \\frac{\\alpha_t(i)\\cdot \\beta_t(i)}{\\sum_j \\alpha_t(j)\\cdot \\beta_t(j)}~;~ \\gamma = \\alpha \\cdot \\beta$$\n",
    "   and\n",
    "   $$\\xi_t(i, j) = \\frac{\\alpha_t(i)\\cdot a_{ij}\\cdot b_j(o_{t+1})\\cdot \\beta_{t+1}(j)}{\\sum_{k,l} \\alpha_t(k)\\cdot a_{k,l}\\cdot b_l(o_{t+1})\\cdot \\beta_{t+1}(l)} $$\n",
    "\n",
    "The goal is to maximize the log-likelihood:\n",
    "$$\\max_{\\lambda} L(\\lambda, \\lambda^{\\star}) = \\max_{\\lambda} \\mathbb{E}_{P(S|O, \\lambda^{\\star})}[\\log P(O, S| \\lambda)]$$\n",
    "\n",
    "with, \n",
    "$$L(\\lambda, \\lambda^{\\star}) = \\sum_{S} P(S|O, \\lambda^{\\star}) \\log P(O, S| \\lambda)$$\n",
    "by calculating the gradient w.r.t $\\lambda = (\\pi, A, B)$, we obtain, for $D$ trajectories\n",
    "\n",
    "$$\\boxed{\\pi_i = \\frac{\\sum_{d=1}^{D} \\gamma_1^{d}(i)}{D}~,~ a_{ij} = \\frac{\\sum_{d=1}^{D}\\sum_{t=1}^{T-1} \\xi_t^{d}(i, j)}{\\sum_{d=1}^{D}\\sum_{t=1}^{T-1}\\gamma_t^d(i)}~,~ b_j(k) = \\frac{\\sum_{d=1}^{D}\\sum_{t=1, o_t^d=k}^{T}\\gamma_t^d(j)}{\\sum_{d=1}^{D}\\sum_{t=1}^{T}\\gamma_t^d(j)}}$$\n",
    "\n",
    "the compact expressions are:\n",
    "$$\\boxed{\\pi = \\gamma_1~,~ A = \\frac{\\sum_t \\xi_t}{\\sum_t \\gamma_t} ~,~ B=\\frac{\\sum_t \\gamma_t \\cdot \\mathbb{1}(O_t=\\cdot)}{\\sum_t \\gamma_t}}$$\n",
    "\n",
    "#### (3.) Viterbi algorithm\n",
    "\n",
    "\n",
    "### Implementation details (tensor shape):\n",
    "\n",
    "#### Discrete case:\n",
    "   Denote $s$ the number of states, $o$ the number of observation values; $T$ the length of the observation sequence.\n",
    "   \n",
    "For one (resp. $D$) trajectory (resp. trajectories):\n",
    "- $\\pi$ of shape = (s,)\n",
    "- $A$ of shape = (s, s)  \n",
    "- $B$ of shape = (s, o)\n",
    "- $O$ of shape = (T, )\n",
    "- $S$ of shape = (T, )\n",
    "- $\\alpha$ of shape = (T, s)  (resp. (D, T, s))\n",
    "- $\\beta$ of shape = (T, s)  (resp. (D, T, s))\n",
    "- $\\gamma$ of shape = (T, s)  (resp. (D, T, s))\n",
    "- $\\xi$ of shape = (T-1, s, s)  (resp. (D, T-1, s, s))\n",
    "\n",
    "\n",
    "### Biblio\n",
    "For the derivation of above formula, the following sites are recommended:\n",
    "- [Stanford open-course: CS228 Probabilistic graphical models (V.Kuleshov and S.Ermon)](https://ermongroup.github.io/cs228-notes/)\n",
    "- [CMU open-course: Probabilistic graphical models](http://www.cs.cmu.edu/~epxing/Class/10708-14/lecture.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Highlight (numpy tricks)\n",
    "\n",
    "- In practice, the values of $\\alpha$, $beta$, etc go to zero quickly, thus, we use log-scale version. The function **compute_log_sum** takes $logx = (\\log x_1, \\log x_2, ..., \\log x_T)$ as input and return $$\\log \\sum_i^T x_i$$\n",
    "In order to avoid the numeric value instability, I first *subtract the max*, take *exp*, *sum* them, *log* them and finally *add* back the max.\n",
    "- Compute a subset sum (log version): see the function **update_params**. $$b_j(k) = \\frac{\\sum_{d=1}^{D}\\sum_{t=1, o_t^d=k}^{T}\\gamma_t^d(j)}{\\sum_{d=1}^{D}\\sum_{t=1}^{T}\\gamma_t^d(j)}$$\n",
    "\n",
    "### Overall: I use numpy\n",
    "\n",
    "- **Basic tensor operations**: tensor shape $(a_n, ...,a_2, a_1)$, $+-~\\cdot~/$ tensor of shape $(b_m, ...,b_2, b_1)$ to compute 4-dim tensor **without for loop** (D trajectories version) like $$\\xi_t(i, j) = \\frac{\\alpha_t(i)\\cdot a_{ij}\\cdot b_j(o_{t+1})\\cdot \\beta_{t+1}(j)}{\\sum_{k,l} \\alpha_t(k)\\cdot a_{k,l}\\cdot b_l(o_{t+1})\\cdot \\beta_{t+1}(l)} $$ \n",
    "- **Slice tricks**: A.shape $= (a_1, a_2,...,a_n)$, B.shape $= (b_1, b_2, ...,b_m)$, then \n",
    "$$B[A,...].shape = (a_1, a_2,...,a_n, b_2, ...,b_m)$$\n",
    "under condition that all element of A are contained in $np.arange(b_1)$\n",
    "- Build a multi-dimensional version of *np.random.choice* which allows the probability $p$ to be a tensor.\n",
    "- \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### utils\n",
    "def random_choice_multidim(p=None, size=1):\n",
    "    \"\"\"\n",
    "    Denote N the number of objects (discrete random choice).\n",
    "    Given D distributions p.shape = (D, N), return samples.shape = (D, )\n",
    "    e.g. D = 4, N = 3\n",
    "    p = np.array([[0.2, 0.1, 0.7],\n",
    "                  [0.3, 0.3, 0.4],\n",
    "                  [0.1, 0.5, 0.4],\n",
    "                  [0.7, 0.1, 0.2]])\n",
    "    when p.shape = (N, ), this is equivalent to \n",
    "        np.random.choice(np.arange(N), p=p)\n",
    "    \n",
    "    return samples of shape = (D, ) + size = (D, size)\n",
    "    \"\"\"\n",
    "    if len(p.shape) == 1:\n",
    "        samples = np.random.choice(np.arange(len(p)), p=p, size=size)\n",
    "\n",
    "    else:\n",
    "        assert len(p.shape) == 2, \"p.shape == (D, N); D distributions, N objects\"\n",
    "        assert type(size) == int, \"only support int for now, this could be updated\"\n",
    "        D, N = p.shape\n",
    "        # index = np.random.rand(D, *size) ### uniform sampling from [0, 1] of shape (D, )+size\n",
    "        index = np.random.rand(D, size)\n",
    "        cumsum_p = np.cumsum(p, axis=-1) ## TODO TOO WASTE ?\n",
    "        if size>1:\n",
    "            samples = np.max(np.cumsum(index[...,None]>cumsum_p[:, None, :], axis=-1), axis=-1)\n",
    "        else:\n",
    "            samples = np.max(np.cumsum(index>cumsum_p, axis=-1), axis=-1)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HMM(object):\n",
    "    \"\"\"\n",
    "    A simple HMM model (discrete observation space). \n",
    "    Support D trajectories of observations (Obs.shape = (D, T)).\n",
    "    \n",
    "    init_distr: initial distribution of states shape = (num_s, )\n",
    "    A: transition proba matrix shape = (num_s, num_s)\n",
    "    B: emission proba matrix shape = (num_s, num_o)\n",
    "    T: length of each trajectory\n",
    "    \n",
    "    num_s : number of states\n",
    "    num_o : number of observation values\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, init_distr, A, B, use_log=False):\n",
    "        super(HMM, self).__init__()\n",
    "        self.init_distr = init_distr ## shape = (num_s, )\n",
    "        self.A = A ## shape = (num_s, num_s)\n",
    "        self.B = B ## shape = (num_s, num_o)\n",
    "        self.num_s, self.num_o = self.B.shape\n",
    "        self.use_log = use_log\n",
    "        if self.use_log:\n",
    "            self.init_distr = np.log(init_distr)\n",
    "            self.A = np.log(self.A)\n",
    "            self.B = np.log(self.B)\n",
    "\n",
    "\n",
    "    def compute_log_sum(self, X, axis=0):\n",
    "        \"\"\"\n",
    "        Suppose that X = (log x1, log x2, ..., log xn) is already in log-scale.\n",
    "        We want log(sum x_i). First, we substract by max (the most important term), \n",
    "        np.exp, np.sum and np.log.\n",
    "        log x_i -> log(x_i/x_max) -> x_i/x_max -> sum x_i/x_max -> log(sum x_i/x_max)\n",
    "          -> log(sum x_i/x_max) + log x_max\n",
    "\n",
    "        keepdims is always true !\n",
    "        \"\"\"\n",
    "        max_tempo = np.max(X, axis=axis, keepdims=True)\n",
    "        return max_tempo + np.log(np.sum(np.exp(X-max_tempo), axis=axis, keepdims=True))\n",
    "    def forward(self, Obs):\n",
    "        \"\"\"\n",
    "        forward message alpha\n",
    "        Obs: observations of shape (T, )\n",
    "\n",
    "        return alpha, shape = (T, num_s)\n",
    "        \"\"\"\n",
    "        if len(Obs.shape) == 1:\n",
    "            ## only one trajectory\n",
    "            T = len(Obs)\n",
    "            alpha = np.zeros((T, self.num_s))\n",
    "            if self.use_log:\n",
    "                log_A = self.A\n",
    "                log_B = self.B\n",
    "                alpha[0] = self.init_distr + log_B[:, Obs[0]]\n",
    "                for t in range(1, T):\n",
    "                    tempo = alpha[t-1][:, None] + log_A # shape = (num_s, num_s)\n",
    "                    alpha[t] = self.compute_log_sum(tempo, axis=0) + log_B[:, Obs[t]]\n",
    "            else:\n",
    "                alpha[0] = self.init_distr * self.B[:, Obs[0]]\n",
    "                for i in range(1, T):\n",
    "                    alpha[i] = alpha[i-1][None, :].dot(self.A)*self.B[:, Obs[i]]\n",
    "        else:\n",
    "            ## multi-trajectories\n",
    "            D, T = Obs.shape\n",
    "            alpha = np.zeros((D, T, self.num_s))\n",
    "            if self.use_log:\n",
    "                log_A = self.A\n",
    "                log_B = self.B\n",
    "                alpha[:, 0] = self.init_distr + log_B[:, Obs[:, 0]].T ## shape =(num_s, )+(D, num_s)=(D, num_s)\n",
    "                for t in range(1, T):\n",
    "                    tempo = alpha[:, t-1][:, :, None] + log_A # shape = (D, num_s, num_s)\n",
    "                    alpha[:, t] = np.squeeze(self.compute_log_sum(tempo, axis=1)) + (log_B[:, Obs[:, t]].T)\n",
    "            else:\n",
    "                raise ValueError(\"Not implemented error.\")\n",
    "                alpha[0] = self.init_distr * self.B[:, Obs[0]]\n",
    "                for i in range(1, T):\n",
    "                    alpha[i] = alpha[i-1][None, :].dot(self.A)*self.B[:, Obs[i]]\n",
    "\n",
    "\n",
    "        self.alpha = alpha\n",
    "        return self.alpha\n",
    "    def backward(self, Obs):\n",
    "        \"\"\"\n",
    "        Obs: observations of shape (T, )\n",
    "\n",
    "        return beta, shape = (T, num_s)\n",
    "        \"\"\"\n",
    "        if len(Obs.shape) == 1:\n",
    "            ## only one trajectory\n",
    "            T = len(Obs)\n",
    "            beta = np.zeros((T, self.num_s))\n",
    "            if self.use_log:\n",
    "                ### Since log(1) = 0, we don't need the initialization: \"beta[-1] = np.ones(self.num_s)\".\n",
    "                log_A = self.A\n",
    "                log_B = self.B\n",
    "                for t in range(T-2, -1,-1):\n",
    "                    tempo = (beta[t+1])[None, :] +  log_A + log_B[:, Obs[t+1]] # shape = (num_s, num_s)\n",
    "                    beta[t] = np.squeeze(self.compute_log_sum(tempo, axis=1))\n",
    "            else:\n",
    "                beta[-1] = np.ones(self.num_s)\n",
    "                for t in range(T-2, -1,-1):\n",
    "                    beta[t] = self.A.dot(self.B[:, Obs[t+1]]*beta[t+1])\n",
    "\n",
    "        else:\n",
    "            ## multi-trajectories\n",
    "            D, T = Obs.shape\n",
    "            beta = np.zeros((D, T, self.num_s))\n",
    "            if self.use_log:\n",
    "                ### Since log(1) = 0, we don't need the initialization: \"beta[-1] = np.ones(self.num_s)\".\n",
    "                log_A = self.A\n",
    "                log_B = self.B\n",
    "                for t in range(T-2, -1,-1):\n",
    "                    tempo = (beta[:, t+1])[:, None, :] +  log_A + log_B[:, Obs[:, t+1]].T[:, None, :] # shape = (D, num_s, num_s)\n",
    "                    beta[:, t] = np.squeeze(self.compute_log_sum(tempo, axis=-1), axis=-1)\n",
    "            else:\n",
    "                raise ValueError(\"Not implemented error.\")\n",
    "                beta[-1] = np.ones(self.num_s)\n",
    "                for t in range(T-2, -1,-1):\n",
    "                    beta[t] = self.A.dot(self.B[:, Obs[t+1]]*beta[t+1])\n",
    "\n",
    "        self.beta = beta\n",
    "        return self.beta\n",
    "    def _assert_forward_backward_consistency(self, Obs):\n",
    "        self.forward(Obs)\n",
    "        self.backward(Obs)\n",
    "        if not self.use_log:\n",
    "            proba_obs_forward = np.sum(self.alpha[-1])\n",
    "            proba_obs_backward = np.sum(self.init_distr*self.beta[0]*self.B[:, Obs[0]])\n",
    "        else:\n",
    "            proba_obs_forward = np.sum(np.exp(self.alpha[-1]))\n",
    "            proba_obs_backward = np.sum(np.exp(self.init_distr+self.beta[0]+self.B[:, Obs[0]]))\n",
    "        assert np.allclose(proba_obs_forward, proba_obs_backward), \"Find forward: {}, backward: {}\".format(proba_obs_forward, proba_obs_backward)\n",
    "        return True\n",
    "    def compute_gamma_zeta(self, Obs):\n",
    "        \"\"\"\n",
    "        No \"for loop\" implementation for conditional proba !\n",
    "        gamma: P(i_t = q_i| O, lambda), shape = (T, num_s)\n",
    "        zeta : P(i_t = q_i, i_t+1 = q_j| O, lambda), shape = (T, num_s, num_s)\n",
    "\n",
    "        \"\"\"\n",
    "        if len(Obs.shape) == 1:\n",
    "            ## only one trajectory\n",
    "            if self.use_log:\n",
    "                gamma = self.alpha+self.beta ## shape = (T, num_s) \n",
    "                # max_state = np.max(gamma, axis=1, keepdims=True) ## shape = (T, 1)\n",
    "                # denominator = max_state + np.log(np.sum(np.exp(gamma - max_state), axis=1, keepdims=True)) ## shape = (T,1)\n",
    "                denominator = self.compute_log_sum(gamma, axis=1)\n",
    "                gamma = gamma-denominator\n",
    "\n",
    "                part1 = self.alpha[..., None]+self.A\n",
    "                part2 = self.B.T[Obs[:],:]+self.beta ## shape = (T, num_s)\n",
    "                \n",
    "                part1 = part1[:-1] # shape = (T-1, num_s, num_s)\n",
    "                part2 = part2[1:] # shape = (T-1, num_s)\n",
    "\n",
    "                zeta = part1+part2[:, None, :]## shape = (T-1, num_s, num_s)\n",
    "                # max_states = np.max(zeta, axis=(-2,-1), keepdims=True) ## shape (T-1, 1, 1)\n",
    "                # denominator = max_states + np.log(np.sum(np.exp(zeta - max_states), axis=(1,2), keepdims=True))\n",
    "                denominator = self.compute_log_sum(zeta, axis=(1,2))\n",
    "                zeta = zeta-denominator ## shape = (T-1, num_s, num_s)\n",
    "            else:\n",
    "                gamma = self.alpha*self.beta ## shape = (T, num_s)\n",
    "                gamma = gamma / np.sum(gamma, axis=1, keepdims=True) ## shape = (T, num_s)\n",
    "                \n",
    "                part1 = self.alpha[..., None]*self.A ## shape = (T, num_s, num_s), since (T, N, 1) * (N, N) = (T, N, N)\n",
    "                part2 = self.B.T[Obs[:],:]*self.beta ## shape = (T, num_s)\n",
    "                \n",
    "                part1 = part1[:-1] # shape = (T-1, num_s, num_s)\n",
    "                part2 = part2[1:] # shape = (T-1, num_s)\n",
    "\n",
    "                zeta = part1*part2[:, None, :]## shape = (T-1, num_s, num_s)\n",
    "                zeta = zeta/np.sum(zeta, axis=(1,2), keepdims=True) ## shape = (T-1, num_s, num_s)\n",
    "        else:\n",
    "            ## multi-trajectories\n",
    "            if self.use_log:\n",
    "                gamma = self.alpha+self.beta ## shape = (D, T, num_s) \n",
    "                # max_state = np.max(gamma, axis=1, keepdims=True) ## shape = (T, 1)\n",
    "                # denominator = max_state + np.log(np.sum(np.exp(gamma - max_state), axis=1, keepdims=True)) ## shape = (T,1)\n",
    "                denominator = self.compute_log_sum(gamma, axis=-1)\n",
    "                gamma = gamma-denominator\n",
    "                # assert np.allclose(np.sum(np.exp(gamma), axis=-1), np.ones(Obs.shape))\n",
    "                part1 = self.alpha[..., None]+self.A ## shape = (D, T, num_s, num_s)\n",
    "                part2 = self.B.T[Obs[:],:]+self.beta ## shape = (D, T, num_s)\n",
    "                \n",
    "                part1 = part1[:, :-1] # shape = (D, T-1, num_s, num_s)\n",
    "                part2 = part2[:, 1:] # shape = (D, T-1, num_s)\n",
    "\n",
    "                zeta = part1+part2[:, :, None, :]## shape = (D, T-1, num_s, num_s)\n",
    "                # max_states = np.max(zeta, axis=(-2,-1), keepdims=True) ## shape (T-1, 1, 1)\n",
    "                # denominator = max_states + np.log(np.sum(np.exp(zeta - max_states), axis=(1,2), keepdims=True))\n",
    "                denominator = self.compute_log_sum(zeta, axis=(-1,-2))\n",
    "                zeta = zeta-denominator ## shape = (T-1, num_s, num_s)\n",
    "            else:\n",
    "                raise ValueError(\"Not implemented error.\")\n",
    "                gamma = self.alpha*self.beta ## shape = (T, num_s)\n",
    "                gamma = gamma / np.sum(gamma, axis=1, keepdims=True) ## shape = (T, num_s)\n",
    "                \n",
    "                part1 = self.alpha[..., None]*self.A ## shape = (T, num_s, num_s), since (T, N, 1) * (N, N) = (T, N, N)\n",
    "                part2 = self.B.T[Obs[:],:]*self.beta ## shape = (T, num_s)\n",
    "                \n",
    "                part1 = part1[:-1] # shape = (T-1, num_s, num_s)\n",
    "                part2 = part2[1:] # shape = (T-1, num_s)\n",
    "\n",
    "                zeta = part1*part2[:, None, :]## shape = (T-1, num_s, num_s)\n",
    "                zeta = zeta/np.sum(zeta, axis=(1,2), keepdims=True) ## shape = (T-1, num_s, num_s)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.zeta = zeta\n",
    "        return gamma, zeta\n",
    "\n",
    "    def update_params(self, Obs):\n",
    "        \"\"\"\n",
    "        Update parameters using forward/backward estimation: init_distr, A, B\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if len(Obs.shape) == 1:\n",
    "            ## only one trajectory\n",
    "            if self.use_log:\n",
    "                new_init_distr = self.gamma[0]\n",
    "                part1 = np.squeeze(self.compute_log_sum(self.zeta, axis=0)) ## shape = (num_s, num_s)\n",
    "                part2 = np.squeeze(self.compute_log_sum(self.gamma[:-1], axis=0))[:, None] ## shape (num_s, 1)\n",
    "                new_A = part1 - part2\n",
    "\n",
    "                tempo_index = Obs[:, None] == np.arange(self.num_o) ## shape = (T, num_o)\n",
    "                denominator = self.compute_log_sum(self.gamma, axis=0)\n",
    "                ## compute_log_sum for numerator\n",
    "                numerator_max = np.max(self.gamma, axis=0) ## shape = (s, )\n",
    "                numerator_ori = np.exp(self.gamma-numerator_max) ## shape = (T, s)\n",
    "                numerator = numerator_ori.T.dot(tempo_index) ## shape = (s, o) # sum over index (Obs == k) and time T \n",
    "                numerator = np.log(numerator)+numerator_max[:, None] ## RuntimeWarning: divide by zero encountered in log\n",
    "\n",
    "                new_B = numerator-np.squeeze(denominator)[:, None]\n",
    "                \n",
    "            else:\n",
    "\n",
    "                new_init_distr = self.gamma[0]\n",
    "                new_A = np.sum(self.zeta, axis=0)/(np.sum(self.gamma[:-1], axis=0)[:, None])\n",
    "\n",
    "                tempo_index = Obs[:, None] == np.arange(self.num_o) ## shape = (T, num_o)\n",
    "                new_B = self.gamma.T.dot(tempo_index)\n",
    "                new_B = new_B/(np.sum(self.gamma, axis=0)[:, None])\n",
    "        else:\n",
    "            ## multi-trajectories\n",
    "            if self.use_log:\n",
    "                new_init_distr = np.squeeze(self.compute_log_sum(self.gamma[:, 0], axis=0)) - np.log(len(Obs))\n",
    "                part1 = np.squeeze(self.compute_log_sum(self.zeta, axis=(1, 0)), axis=(1,0)) ## shape = (num_s, num_s)\n",
    "                part2 = np.squeeze(self.compute_log_sum(self.gamma[:, :-1], axis=(1, 0)))[..., None] ## shape (num_s, 1)\n",
    "                new_A = part1 - part2\n",
    "\n",
    "                tempo_index = Obs[..., None] == np.arange(self.num_o) ## shape = (D, T, num_o)\n",
    "                denominator = self.compute_log_sum(self.gamma, axis=(1,0)) ## shape = (1, 1, num_s)\n",
    "                ## compute_log_sum for numerator\n",
    "                numerator_max = np.max(self.gamma, axis=1) ## shape = (D, num_s)\n",
    "                numerator_ori = np.exp(self.gamma-numerator_max[:, None, :]) ## shape = (D, T, num_s)\n",
    "                \n",
    "                numerator = np.sum(numerator_ori[..., None]*tempo_index[:,:,None,:], axis=1) ## shape = (D, num_s, num_o) # sum over index (Obs == k) and time T \n",
    "\n",
    "                numerator = np.log(numerator)+numerator_max[..., None] ## RuntimeWarning: divide by zero encountered in log\n",
    "\n",
    "                new_B = np.squeeze(self.compute_log_sum(numerator, axis=0))-np.squeeze(denominator)[:, None]\n",
    "                \n",
    "            else:\n",
    "                raise ValueError(\"Not implemented error.\")\n",
    "                new_init_distr = self.gamma[0]\n",
    "                new_A = np.sum(self.zeta, axis=0)/(np.sum(self.gamma[:-1], axis=0)[:, None])\n",
    "\n",
    "                tempo_index = Obs[:, None] == np.arange(self.num_o) ## shape = (T, num_o)\n",
    "                new_B = self.gamma.T.dot(tempo_index)\n",
    "                new_B = new_B/(np.sum(self.gamma, axis=0)[:, None])\n",
    "\n",
    "        self.init_distr = new_init_distr\n",
    "        self.A = new_A\n",
    "        self.B = new_B\n",
    "        return new_init_distr, new_A, new_B\n",
    "\n",
    "    def generate_trajectories(self, length, traj=1, init_distr=None, A=None, B=None): # \n",
    "        \"\"\"\n",
    "        length: time T of trajectories\n",
    "        init_distr.shape = (s,)\n",
    "        A.shape = (s, s): transition proba\n",
    "        B.shape = (s, o): emission proba\n",
    "        traj: number of trajectories\n",
    "        \n",
    "        if init_distr=None, A=None, B=None, then use self.A,B,init_distr\n",
    "\n",
    "        return observations.shape = (traj, length)\n",
    "        \"\"\"\n",
    "        #     s = len(init_distr)\n",
    "        if init_distr is None:\n",
    "            if self.use_log:\n",
    "                init_distr = np.exp(self.init_distr)\n",
    "            else:\n",
    "                init_distr = self.init_distr\n",
    "        if A is None:\n",
    "            if self.use_log:\n",
    "                A = np.exp(self.A)\n",
    "            else:\n",
    "                A = self.A\n",
    "        if B is None:\n",
    "            if self.use_log:\n",
    "                B = np.exp(self.B)\n",
    "            else:\n",
    "                B = self.B\n",
    "\n",
    "        s, o = B.shape\n",
    "        observations = []\n",
    "        state = np.random.choice(np.arange(s), p=init_distr, size=(traj,))\n",
    "        obs = random_choice_multidim(p=B[state], size=1)\n",
    "\n",
    "        observations.append(obs[:, None])\n",
    "        for i in range(1, length):\n",
    "            state = random_choice_multidim(p=A[state], size=1)\n",
    "            obs = random_choice_multidim(p=B[state], size=1)\n",
    "            \n",
    "            observations.append(obs[:, None])\n",
    "        return np.concatenate(observations, axis=1)\n",
    "\n",
    "    def main(self, iterations, Obs, verbose=0, A_gt=None, init_distr_gt=None):\n",
    "        # print(np.sum(np.abs(A_gt-np.exp(self.A))))\n",
    "        # print(\"=\"*10)\n",
    "        for i in range(iterations):\n",
    "            \n",
    "            alpha = self.forward(Obs)\n",
    "            beta = self.backward(Obs)\n",
    "            self.compute_gamma_zeta(Obs)\n",
    "            init_distr_es, A_es, B_es = self.update_params(Obs)\n",
    "            if verbose:\n",
    "                if self.use_log:\n",
    "                    alpha = np.exp(alpha)\n",
    "                    beta = np.exp(beta)\n",
    "                    # init_distr = np.exp(self.init_distr)\n",
    "                    # B = np.exp(self.B)\n",
    "                    # A = np.exp(self.A)\n",
    "                else:\n",
    "                    alpha = alpha ## shape = (D, T, num_s)\n",
    "                    beta = beta  ## shape = (D, T, num_s)\n",
    "                    # init_distr = self.init_distr\n",
    "                    # B = self.B\n",
    "                    # A = self.A\n",
    "                # forward_proba = np.mean(np.sum(alpha[:, -1], axis=-1))\n",
    "                # backward_proba = np.sum(init_distr*B[:, Obs[0, 0]]*beta[0, 0])\n",
    "                # print(forward_proba, backward_proba)\n",
    "                # print(forward_proba)\n",
    "                print(np.sum(np.abs(A_gt-np.exp(A_es))), np.sum(np.abs(init_distr_gt-np.exp(init_distr_es))))\n",
    "                # print()\n",
    "                # print(np.sum(np.abs(self.A-A_es)))\n",
    "                \n",
    "    def _sanity_check(self, use_log=True):\n",
    "        self.use_log = use_log ## BAD, TODO\n",
    "\n",
    "        init_distr = np.array([0.2,0.4,0.4])\n",
    "        A = np.array([[0.5,0.2,0.3], [0.3,0.5,0.2], [0.2,0.3,0.5]])\n",
    "        B = np.array([[0.5,0.5], [0.4,0.6], [0.7,0.3]])\n",
    "        obs = np.array([0,1,0])\n",
    "\n",
    "        alpha = self.forward(obs)\n",
    "        beta = self.backward(obs)\n",
    "        gamma, zeta = self.compute_gamma_zeta(obs)\n",
    "        if use_log:\n",
    "            alpha = np.exp(alpha)\n",
    "            beta = np.exp(beta)\n",
    "            gamma = np.exp(gamma)\n",
    "            zeta = np.exp(zeta)\n",
    "        else:\n",
    "            alpha = alpha ## shape = (T, num_s)\n",
    "            beta = beta  ## shape = (T, num_s)\n",
    "            gamma = gamma ## shape = (T, num_s)\n",
    "            zeta = zeta  ## shape = (T-1, num_s, num_s)\n",
    "        print(\"The proba of observation sequence (using forward message): {}\".format(np.sum(alpha[-1])))\n",
    "        print(\"The proba of observation sequence (using backward message): {}\".format(np.sum(init_distr*B[:, obs[0]]*beta[0])))\n",
    "        print(\"The exact proba of observation sequence [0,1,0] is: 0.13022\")\n",
    "        print(\"Forward and backward are consistent: {}\".format(self._assert_forward_backward_consistency(obs)))\n",
    "        assert np.allclose(np.sum(gamma, axis=-1), np.ones(len(gamma))), \"gamma is wrong (not normalized)\"\n",
    "        assert np.allclose(np.sum(zeta, axis=(-1,-2)), np.ones(len(zeta))), \"zeta is wrong (not normalized)\"\n",
    "        assert np.allclose(np.sum(alpha[-1]), np.sum(init_distr*B[:, obs[0]]*beta[0])), \"forward/backward inconsistent\"\n",
    "\n",
    "        alpha_gt = np.array([[0.1, 0.16, 0.28],\n",
    "                             [0.077, 0.1104, 0.0606],\n",
    "                             [0.04187, 0.035512, 0.052836]])\n",
    "        beta_gt = np.array([[0.2451, 0.2622, 0.2277],\n",
    "                             [0.54, 0.49, 0.57],\n",
    "                             [1., 1., 1.]])\n",
    "        assert np.allclose(alpha, alpha_gt), \"alpha is wrong\"\n",
    "        assert np.allclose(beta, beta_gt), \"beta is wrong\"\n",
    "        print('All passed!')\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: the evaluation problem -- find $P(O|\\lambda)$:\n",
    "Below, I give two toy examples with which the answers can be easily (manually) verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The proba of observation sequence (using forward message): 0.13021799999999997\n",
      "The proba of observation sequence (using backward message): 0.13021799999999997\n",
      "The exact proba of observation sequence [0,1,0] is: 0.13022\n",
      "Forward and backward are consistent: True\n",
      "All passed!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_log = True ## False\n",
    "init_distr = np.array([0.2,0.4,0.4])\n",
    "A = np.array([[0.5,0.2,0.3], [0.3,0.5,0.2], [0.2,0.3,0.5]])\n",
    "B = np.array([[0.5,0.5], [0.4,0.6], [0.7,0.3]])\n",
    "# obs = np.array([0,1,0]) \n",
    "algo = HMM(init_distr, A, B, use_log=use_log)\n",
    "algo._sanity_check(use_log=use_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Example 1\n",
      "====================\n",
      "The proba of observation sequence (using forward message): 0.13021799999999997\n",
      "The proba of observation sequence (using backward message): 0.13021799999999997\n",
      "The exact proba of observation sequence [0,1,0] is: 0.13022\n",
      "Forward and backward are consistent: True\n",
      "====================\n",
      "Example 2 (from Wiki)\n",
      "====================\n",
      "Observation sequence: [0 1 2]\n",
      "The proba of observation sequence (using forward message): 0.03628\n",
      "The proba of observation sequence (using backward message): 0.03627999999999999\n",
      "=====\n",
      "The exact proba of observation sequence [0] is: 0.4*0.1 + 0.6*0.5 = 0.34\n",
      "The exact proba of observation sequence [0, 1] is: 0.4*0.1*(0.6*0.3+0.4*0.4) + 0.6*0.5*(0.7*0.4+0.3*0.3)=0.1246\n",
      "The exact proba of observation sequence [0, 1, 2] is: 0.03628\n",
      "Forward and backward are consistent: True\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Simple sample\n",
    "##################\n",
    "print(\"=\"*20)\n",
    "print(\"Example 1\")\n",
    "print(\"=\"*20)\n",
    "use_log = True\n",
    "init_distr = np.array([0.2,0.4,0.4])\n",
    "A = np.array([[0.5,0.2,0.3], [0.3,0.5,0.2], [0.2,0.3,0.5]])\n",
    "B = np.array([[0.5,0.5], [0.4,0.6], [0.7,0.3]])\n",
    "obs = np.array([0,1,0]) # ,0,0,0,0,1,1,1,1,1\n",
    "\n",
    "algo = HMM(init_distr, A, B, use_log=use_log)\n",
    "if use_log:\n",
    "    alpha = np.exp(algo.forward(obs))\n",
    "    beta = np.exp(algo.backward(obs))\n",
    "else:\n",
    "    alpha = algo.forward(obs)\n",
    "    beta = algo.backward(obs)\n",
    "print(\"The proba of observation sequence (using forward message): {}\".format(np.sum(alpha[-1])))\n",
    "print(\"The proba of observation sequence (using backward message): {}\".format(init_distr.dot(beta[0]*B[:, obs[0]])))\n",
    "print(\"The exact proba of observation sequence [0,1,0] is: 0.13022\")\n",
    "print(\"Forward and backward are consistent: {}\".format(algo._assert_forward_backward_consistency(obs)))\n",
    "\n",
    "\n",
    "##################\n",
    "## Wiki sample\n",
    "##################\n",
    "print(\"=\"*20)\n",
    "print(\"Example 2 (from Wiki)\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "init_distr = np.array([0.6,0.4])\n",
    "A = np.array([[0.7,0.3], [0.4,0.6]])\n",
    "B = np.array([[0.5,0.4,0.1], [0.1,0.3,0.6]])\n",
    "# obs = np.array([0])\n",
    "# obs = np.array([0,1])\n",
    "obs = np.array([0,1,2])\n",
    "\n",
    "algo = HMM(init_distr, A, B, use_log=use_log)\n",
    "if use_log:\n",
    "    alpha = np.exp(algo.forward(obs))\n",
    "    beta = np.exp(algo.backward(obs))\n",
    "else:\n",
    "    alpha = algo.forward(obs)\n",
    "    beta = algo.backward(obs)\n",
    "print(\"Observation sequence: {}\".format(obs))\n",
    "print(\"The proba of observation sequence (using forward message): {}\".format(np.sum(alpha[-1])))\n",
    "print(\"The proba of observation sequence (using backward message): {}\".format(init_distr.dot(beta[0]*B[:, obs[0]])))\n",
    "print(\"=\"*5)\n",
    "print(\"The exact proba of observation sequence [0] is: 0.4*0.1 + 0.6*0.5 = 0.34\")\n",
    "print(\"The exact proba of observation sequence [0, 1] is: 0.4*0.1*(0.6*0.3+0.4*0.4) + 0.6*0.5*(0.7*0.4+0.3*0.3)=0.1246\")\n",
    "print(\"The exact proba of observation sequence [0, 1, 2] is: 0.03628\")\n",
    "print(\"Forward and backward are consistent: {}\".format(algo._assert_forward_backward_consistency(obs)))\n",
    "\n",
    "# print(algo.compute_gamma_zeta(obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: EM algo (with multi-trajectories) -- find $\\lambda= (\\pi, A, B)$\n",
    "We have D trajectories of observations shape = (D, T). The goal is to estimate $\\lambda= (\\pi, A, B)$ by maximizing the log-likilihood using EM algorithm. Here, we use the original version of HMM (i.e. without gaussian kernel). The results are not very pleasing.\n",
    "\n",
    "p.s. Notice that the states index maybe shifted, thus the order of matrice $A$, $B$ may be shuffled.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3534836  0.38473197 0.26178442]\n",
      "[[0.3376784  0.30045706 0.36186454]\n",
      " [0.27651327 0.5556333  0.16785343]\n",
      " [0.1624254  0.25890656 0.57866804]]\n",
      "[[0.50557414 0.49442586]\n",
      " [0.4232548  0.5767452 ]\n",
      " [0.6740665  0.3259335 ]]\n"
     ]
    }
   ],
   "source": [
    "init_distr = np.array([0.2,0.4,0.4])\n",
    "A = np.array([[0.5,0.2,0.3], [0.3,0.5,0.2], [0.2,0.3,0.5]])\n",
    "B = np.array([[0.5,0.5], [0.4,0.6], [0.7,0.3]])\n",
    "algo = HMM(init_distr, A, B, use_log=True)\n",
    "obs_traj = algo.generate_trajectories(200, traj=100)\n",
    "\n",
    "\n",
    "A = A + 0.5*np.random.random((3,3))\n",
    "A = A/np.sum(A, axis=1, keepdims=True)\n",
    "B = B + 0.5*np.random.random((3, 2))\n",
    "B = B/np.sum(B, axis=1, keepdims=True)\n",
    "init_distr = init_distr + 0.5*np.random.random(3)\n",
    "init_distr = init_distr/np.sum(init_distr)\n",
    "algo = HMM(init_distr, A, B, use_log=True)\n",
    "\n",
    "algo.main(100, obs_traj)\n",
    "print(np.exp(algo.init_distr))\n",
    "print(np.exp(algo.A))\n",
    "print(np.exp(algo.B))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py36",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
