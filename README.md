# Probabilistic_Graphical_Models
python implementation of classic probabilistic graphical models 


## HMM (Hidden Markov Model)
I implement a tensor version (using numpy) of HMM for three classic problems of HMM. **For any non-sequential/non-recursive, I bypass the for-loop by using tensor operations.** Thus, my numpy implementation could be easily adapted by tensorflow, pytorch.

1. The Evaluation problem: (forward/backward messages) Given ğœ†=(ğœ‹,ğ´,ğµ)  and a sequence of observation  ğ‘‚=(ğ‘œ1,ğ‘œ2,ğ‘œğ‘‡) , estimate  P(ğ‘‚|ğœ†).

2. The Learning problem: (Baum-Welch algorithm, aka EM-algorithm) Given  ğ‘‚=(ğ‘œ1,ğ‘œ2,ğ‘œğ‘‡) , estimate best fit parameters  ğœ†=(ğœ‹,ğ´,ğµ).

3. The Decoding problem: (Viterbi's algorithm) Given  ğœ†=(ğœ‹,ğ´,ğµ)  and a sequence of observation  ğ‘‚=(ğ‘œ1,ğ‘œ2,ğ‘œğ‘‡) , estimate hidden states.


Please see directly the jupyter notebook HMM.ipynb for more details.

Project ongoing: (check list)

- tensor version of basic HMM ...done.
- discrete observation space...done.
- D trajectories version of HMM...done.
- Problem 1, 2...done.
- Problem 3...partially.
- continue version (with gaussian kernel) ... partially.
- D trajectories, continue version
- Monitor likelihood in the Problem 2.





